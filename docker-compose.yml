version: '3.8'

services:
  # vLLM Server for mem-agent (using official image)
  # Provides OpenAI-compatible API for LLM inference
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: tg-note-vllm
    restart: unless-stopped
    env_file: .env
    command: >
      --model ${MEM_AGENT_MODEL:-driaforall/mem-agent}
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.8}
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
    ports:
      - "${VLLM_PORT:-8001}:8001"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - tg-note-network
    shm_size: '2gb'

  # Docling MCP server (Dockerized Docling toolchain)
  # Provides document processing with multiple OCR backends (RapidOCR, EasyOCR, Tesseract, OnnxTR)
  # and VLM support with GPU acceleration
  docling-mcp:
    build:
      context: .
      dockerfile: docker/docling-mcp/Dockerfile
    image: tg-note-docling-mcp:latest
    container_name: tg-note-docling
    restart: unless-stopped
    env_file: .env
    environment:
      # Logging configuration
      - DOCLING_LOG_LEVEL=${DOCLING_LOG_LEVEL:-INFO}
      # Directory paths inside container
      - DOCLING_MODELS_DIR=/opt/docling-mcp/models
      - DOCLING_CACHE_DIR=/opt/docling-mcp/cache
      - DOCLING_LOG_DIR=/opt/docling-mcp/logs
      # HuggingFace cache for model downloads
      - HF_HOME=/opt/docling-mcp/cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=${DOCLING_CUDA_VISIBLE_DEVICES:-all}
      - NVIDIA_VISIBLE_DEVICES=${DOCLING_CUDA_VISIBLE_DEVICES:-all}
      # OCR backend configuration (can be overridden by config file)
      - DOCLING_OCR_BACKEND=${DOCLING_OCR_BACKEND:-rapidocr}
    ports:
      - "${DOCLING_MCP_PORT:-8077}:8077"
    healthcheck:
      test: ["CMD-SHELL", "bash -c '</dev/tcp/127.0.0.1/8077' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    volumes:
      # Shared configuration file
      - ./config.yaml:/opt/docling-mcp/config.yaml:ro
      # Models cache directory (persistent across restarts)
      - ./data/docling/models:/opt/docling-mcp/models
      # General cache directory (HuggingFace, ModelScope, etc.)
      - ./data/docling/cache:/opt/docling-mcp/cache
      # Logs directory
      - ./logs/docling:/opt/docling-mcp/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${DOCLING_GPU_COUNT:-all}
              capabilities: [gpu]
    networks:
      - tg-note-network
    shm_size: '2gb'

  # MCP Hub Server (Unified MCP Gateway)
  # Provides:
  # - Built-in MCP tools (memory)
  # - MCP server registry
  # - HTTP/SSE interface
  mcp-hub:
    build:
      context: .
      dockerfile: Dockerfile.hub
    container_name: tg-note-hub
    image: tg-note-hub:latest
    restart: unless-stopped
    env_file: .env
    ports:
      - "${MCP_PORT:-8765}:8765"
    volumes:
      # Persistent storage for memory data
      - ./data/memory:/app/data/memory
      # MCP servers registry
      - ./data/mcp_servers:/app/data/mcp_servers
      # Logs
      - ./logs:/app/logs
      # Configuration (read-only)
      - ./config.yaml:/app/config.yaml:ro
      # Shared HuggingFace cache for embeddings (bind host cache)
      - ~/.cache/huggingface:/root/.cache/huggingface
    depends_on:
      vllm-server:
        condition: service_healthy
      docling-mcp:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - tg-note-network

  # Main Telegram Bot
  bot:
    build:
      context: .
      dockerfile: Dockerfile.bot
    container_name: tg-note-bot
    image: tg-note-bot:latest
    restart: unless-stopped
    env_file: .env
    environment:
      # MCP Hub URL for Docker mode (bot connects to hub via HTTP/SSE)
      - MCP_HUB_URL=http://mcp-hub:8765/sse
    volumes:
      # Persistent storage for knowledge base
      - ./knowledge_base:/app/knowledge_base
      # Persistent storage for data (processed messages, settings)
      - ./data:/app/data
      # Logs
      - ./logs:/app/logs
      # Configuration (read-only)
      - ./config.yaml:/app/config.yaml:ro
      # Persist Qwen CLI auth and settings between restarts
      - ~/.qwen:/root/.qwen
    depends_on:
      mcp-hub:
        condition: service_healthy
    networks:
      - tg-note-network

networks:
  tg-note-network:
    driver: bridge

volumes:
