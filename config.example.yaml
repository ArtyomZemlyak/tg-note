# ═══════════════════════════════════════════════════════════════════════════════
# Example Application Configuration
# ═══════════════════════════════════════════════════════════════════════════════
# IMPORTANT: Copy this file to config.yaml and adjust according to your needs
#
# This file contains main application settings that do not include sensitive data.
#
# All sensitive data (tokens, API keys, passwords) should be stored in .env file
# or passed through environment variables to ensure security.
# ═══════════════════════════════════════════════════════════════════════════════


# ───────────────────────────────────────────────────────────────────────────────
# Knowledge Base Settings
# ───────────────────────────────────────────────────────────────────────────────
# The knowledge base is used to store and manage documents processed by the bot.
# Git integration is supported for versioning.

# KB_PATH: Root directory for all knowledge bases
# - This is the base directory where all user knowledge bases are stored
# - Each user's KB is created as a subdirectory: KB_PATH/my-notes, KB_PATH/work-kb, etc.
# - Can be relative (./knowledge_base) or absolute (/path/to/kbs)
# - Directory will be created automatically if it doesn't exist
KB_PATH: ./knowledge_base

# KB_TOPICS_ONLY: Restrict agents to work only in topics/ folder
# - true: Agents can only create/edit/read files in {kb_name}/topics/ directory
#         This prevents agents from modifying index.md, README.md, and other meta files
#         in the knowledge base root. Recommended for production use.
# - false: Agents have full access to entire knowledge base directory
#          Useful for testing or if you need agents to manage meta files
KB_TOPICS_ONLY: true

# KB_GIT_ENABLED: Enable Git integration
# - true: All changes in the knowledge base will be committed to Git repository
# - false: Knowledge base works as regular file system without versioning
KB_GIT_ENABLED: true

# KB_GIT_AUTO_PUSH: Automatically push changes to remote repository
# - true: After each commit, changes are automatically pushed to remote server
# - false: Commits remain local, manual synchronization required
# - IMPORTANT: Only works if KB_GIT_ENABLED=true
KB_GIT_AUTO_PUSH: true

# KB_GIT_REMOTE: Name of the remote Git repository
# - Default "origin" - standard name for the main remote repository
# - Can be changed to another name if using non-standard setup
KB_GIT_REMOTE: origin

# KB_GIT_BRANCH: Git branch for working with knowledge base
# - All commits will be created and pushed to this branch
# - Make sure the branch exists or will be created
KB_GIT_BRANCH: main


# ───────────────────────────────────────────────────────────────────────────────
# Processing Settings
# ───────────────────────────────────────────────────────────────────────────────
# Parameters controlling the logic of processing incoming messages and their grouping.

# MESSAGE_GROUP_TIMEOUT: Message grouping timeout (in seconds)
# - Wait time before processing a group of messages
# - If user sends multiple messages in a row, bot will wait specified number of
#   seconds to group them and process together
# - Recommended value: 30-60 seconds
# - Minimum: 10 seconds, maximum: 300 seconds (5 minutes)
MESSAGE_GROUP_TIMEOUT: 30  # seconds

# PROCESSED_LOG_PATH: Path to the processed messages log file
# - JSON file for tracking history of processed messages
# - Used to prevent duplicate processing and recovery after failures
# - File is created automatically, directory must exist
PROCESSED_LOG_PATH: ./data/processed.json


# ───────────────────────────────────────────────────────────────────────────────
# Logging Settings
# ───────────────────────────────────────────────────────────────────────────────
# Logging system configuration for debugging and monitoring bot operations.

# LOG_LEVEL: Log detail level
# Available values (from least detailed to most detailed):
#
# - CRITICAL: Only critical errors that lead to application shutdown
#
# - ERROR:   Errors that break function operations but don't stop the application
#
# - WARNING: Warnings about potential issues (recommended for production)
#
# - INFO:    General information about application flow (default)
#
# - DEBUG:   Detailed information for debugging, including variables and traces
#            WARNING: May contain sensitive data, do not use in production
#
# Recommendations:
# - Development/debugging: DEBUG or INFO
# - Production: WARNING or ERROR
LOG_LEVEL: INFO

# LOG_FILE: Path to log file
# - All logs are written to this file
# - Automatic file rotation is supported when reaching certain size
# - Make sure directory exists and application has write permissions
# - Log format: timestamp, level, module, message
LOG_FILE: ./logs/bot.log


# ───────────────────────────────────────────────────────────────────────────────
# Media Processing Settings
# ───────────────────────────────────────────────────────────────────────────────
# Configuration for file format recognition and content extraction.
# Controls which file formats are processed by different media processing frameworks.

# MEDIA_PROCESSING_ENABLED: Master switch for media file processing
#
# - true: Enable media file processing (default)
#         Files sent to the bot will be processed according to enabled formats
#         Content extraction works with Docling and other configured frameworks
#
# - false: Disable all media file processing
#          Files sent to the bot will be ignored
#          Useful if you want to temporarily disable this feature
#          or reduce resource usage
#
# NOTE: When disabled, MEDIA_PROCESSING_*_FORMATS settings are ignored
MEDIA_PROCESSING_ENABLED: true

# MEDIA_PROCESSING_DOCLING: Docling-specific configuration (formats, limits, OCR)
#
# Docling extracts text and structure from PDFs, Office documents, images (OCR), and more.
# The configuration below allows you to control granular behaviour:
#
# enabled:
#   - true  (default)  — Docling is active when MEDIA_PROCESSING_ENABLED is true
#   - false            — Docling is completely disabled
#
# formats:
#   - List of file extensions Docling should accept (lowercase without dots)
#   - Default includes documents, text files, and common image formats
#
# max_file_size_mb:
#   - Maximum file size Docling will process (in megabytes)
#   - Set to 0 to remove the limit
#
# prefer_markdown_output:
#   - true  (default) — Prefer Markdown export when both Markdown and plain text are available
#   - false           — Prefer plain text export when available
#
# fallback_plain_text:
#   - true  (default) — If preferred export fails, try the alternative exporter automatically
#   - false           — Do not fallback; return empty content when preferred export fails
#
# image_ocr_enabled:
#   - true  (default) — Allow OCR for image formats (jpg/jpeg/png/tiff)
#   - false           — Skip image files even if listed in formats
#
# ocr_languages:
#   - ISO 639-3 language codes for OCR engine hints (Docling passes these to OCR backend)
#
# keep_images:
#   - Preserve page images generated by Docling (can increase storage usage)
#
# generate_page_images:
#   - Request page images even when keep_images is false (used for visual previews)
#
# startup_sync:
#   - true  (default) — Docling container downloads required models on startup
#   - false           — Skip automatic sync (загрузка выполняется только при изменении настроек)
#
# mcp:
#   - timeout: Timeout for Docling MCP requests in seconds (default: 180 for document/image processing)
#             Increase this if processing large files or complex documents with OCR
#
# ocr_config:
#   - backend: rapidocr | easyocr | tesseract | tesseract_cli | onnxtr | none
#   - languages: Override OCR languages (falls back to ocr_languages when empty)
#   - rapidocr/easyocr/tesseract/onnxtr: Backend-specific options (model paths, providers, etc.)
#
# model_cache:
#   - base_dir: Container-side model directory (/opt/docling-mcp/models by default)
#   - builtin_models: Predefined Docling bundles to download via docling utils
#   - downloads: Additional artefacts (HuggingFace / ModelScope) downloaded manually
#   - pipeline: Feature switches that control which Docling stages run (tables, VLM, etc.)
#
# Legacy support: MEDIA_PROCESSING_DOCLING_FORMATS is still recognized but considered deprecated.
MEDIA_PROCESSING_DOCLING:
  enabled: true
  max_file_size_mb: 25
  prefer_markdown_output: true
  fallback_plain_text: true
  image_ocr_enabled: true
  keep_images: false
  generate_page_images: false
  startup_sync: true
  mcp:
    timeout: 180  # Timeout for Docling MCP requests (seconds). Increase for large files.
  ocr_languages:
    - eng  # English (ISO 639-3)
    - rus  # Russian (ISO 639-3)
  ocr_config:
    backend: rapidocr
    languages: []  # Empty = use ocr_languages as fallback
    rapidocr:
      providers:
        - CUDAExecutionProvider
        - CPUExecutionProvider
      repo_id: RapidAI/RapidOCR
      det_model_path: RapidOcr/onnx/PP-OCRv4/det/ch_PP-OCRv4_det_infer.onnx
      rec_model_path: RapidOcr/onnx/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.onnx
      cls_model_path: RapidOcr/onnx/PP-OCRv4/cls/ch_ppocr_mobile_v2.0_cls_infer.onnx
      rec_keys_path: RapidOcr/paddle/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer/ppocr_keys_v1.txt
    easyocr:
      enabled: false
      languages:
        - en  # English (ISO 639-1)
        - ru  # Russian (ISO 639-1)
      gpu: auto
    tesseract:
      enabled: false
      languages:
        - eng  # English (ISO 639-3)
        - rus  # Russian (ISO 639-3)
    onnxtr:
      enabled: false
  formats:
    - pdf
    - docx
    - pptx
    - xlsx
    - html
    - md
    - txt
    - jpg
    - jpeg
    - png
    - tiff
  model_cache:
    base_dir: /opt/docling-mcp/models
    builtin_models:
      layout: true
      tableformer: true
      code_formula: true
      picture_classifier: true
      rapidocr:
        enabled: true
        backends:
          - onnxruntime
      easyocr: false
      smolvlm: false
      granitedocling: false
      granitedocling_mlx: false
      smoldocling: false
      smoldocling_mlx: false
      granite_vision: false
    downloads: []
  pipeline:
    layout:
      enabled: true
      preset: layout_v2
    table_structure:
      enabled: true
      mode: accurate
      do_cell_matching: true
    code_enrichment: false
    formula_enrichment: false
    picture_classifier: false
    picture_description:
      enabled: false
      model: smolvlm

# Deprecated but supported legacy structure (list-only configuration):
# MEDIA_PROCESSING_DOCLING_FORMATS:
#   - pdf
#   - docx
#   - ...

# AICODE-NOTE: Future media processing frameworks can be added here
# Example structure:
# MEDIA_PROCESSING_SOME_OTHER_FRAMEWORK_FORMATS:
#   - mp3
#   - mp4
#   - wav


# ───────────────────────────────────────────────────────────────────────────────
# User Access Control
# ───────────────────────────────────────────────────────────────────────────────
# Security settings to restrict access to the bot.

# ALLOWED_USER_IDS: List of allowed Telegram user IDs
#
# Format:
# - Empty string ("") - access allowed for all users (unsafe for production!)
#
# - Single ID: "123456789"
#
# - Multiple IDs (comma-separated): "123456789,987654321,555555555"
#
# How to find your Telegram ID:
# 1. Message @userinfobot bot in Telegram
# 2. Or use @getmyid_bot
# 3. ID will be shown in response message
#
# IMPORTANT: In production environment ALWAYS specify specific user IDs!
#
# Example for multiple users:
# ALLOWED_USER_IDS: "123456789,987654321"
ALLOWED_USER_IDS: ''


# ───────────────────────────────────────────────────────────────────────────────
# Agent Configuration
# ───────────────────────────────────────────────────────────────────────────────
# AI agent settings that process user requests and perform tasks.

# AGENT_TYPE: Type of agent to use
#
# Available options:
#
# - "stub": Stub for testing without real AI
#          Returns fixed responses, doesn't require API keys
#          Use only for development and testing
#
# - "qwen_code": Agent based on Qwen AI via API
#               Requires QWEN_API_KEY in .env file
#               Less stable, may have issues with tools
#
# - "qwen_code_cli": Agent based on Qwen AI via command line (RECOMMENDED)
#                   Requires installed qwen CLI tool
#                   Most stable option with full tool support
#                   Supports code, files, web search operations
AGENT_TYPE: qwen_code_cli

# AGENT_MODEL: AI model to use (for qwen_code type)
#
# Available Qwen models:
#
# - "qwen-max": Most advanced model, best response quality
#              Recommended for complex tasks
#
# - "qwen-plus": Balanced model, good quality/speed ratio
#
# - "qwen-turbo": Fast model for simple tasks
#
# NOTE: This setting is only used if AGENT_TYPE="qwen_code"
# For qwen_code_cli the model is set in CLI tool configuration
AGENT_MODEL: qwen-max

# AGENT_QWEN_CLI_PATH: Path to qwen CLI executable
#
# - "qwen": CLI available through PATH (recommended)
#
# - "/usr/local/bin/qwen": Absolute path to executable
#
# - "./bin/qwen": Relative path from project root
#
# Installing qwen CLI:
# 1. Follow instructions at https://github.com/QwenLM/Qwen-Agent
# 2. After installation check: qwen --version
#
# NOTE: Only used if AGENT_TYPE="qwen_code_cli"
AGENT_QWEN_CLI_PATH: qwen

# AGENT_TIMEOUT: Maximum time for agent operation execution (in seconds)
#
# - Prevents hanging during long operations
#
# - If operation exceeds this time, it will be interrupted
#
# - Recommended values:
#   * Simple requests: 60-120 seconds
#   * Complex tasks with code: 300-600 seconds (5-10 minutes)
#   * Very complex processing: 900-1800 seconds (15-30 minutes)
#
# IMPORTANT: Too short timeout may interrupt useful operations
AGENT_TIMEOUT: 300

# AGENT_ENABLE_WEB_SEARCH: Allow agent to use web search
#
# - true: Agent can search information online to answer questions
#         Useful for getting up-to-date information and reference data
#
# - false: Agent works only with local data
#          More secure, but limits capabilities
#
# NOTE: Requires internet connection and may increase response time
AGENT_ENABLE_WEB_SEARCH: true

# AGENT_ENABLE_GIT: Allow agent to work with Git repositories
#
# - true: Agent can perform Git operations (clone, pull, commit, push, etc.)
#         Required for working with versioning and repositories
#
# - false: Git functions disabled
#
# IMPORTANT: Make sure Git is installed in the system and configured
# (git config user.name and git config user.email must be set)
AGENT_ENABLE_GIT: true

# AGENT_ENABLE_GITHUB: Allow agent to interact with GitHub API
#
# - true: Agent can work with GitHub (create issues, PRs, read code, etc.)
#         Requires GITHUB_TOKEN in .env file for authorization
#
# - false: GitHub integration disabled
#
# Usage:
# - Creating and managing issues
# - Working with pull requests
# - Reading code from public and private repositories
# - Analyzing commit history
AGENT_ENABLE_GITHUB: true

# AGENT_ENABLE_SHELL: Allow agent to execute shell commands
#
# ⚠️ CRITICAL SECURITY WARNING ⚠️
#
# - true: Agent can execute ANY commands in the operating system
#         EXTREMELY DANGEROUS! Can lead to:
#         * File and data deletion
#         * System compromise
#         * Unauthorized access
#         * Malicious code execution
#
# - false: Shell commands forbidden (RECOMMENDED)
#
# IMPORTANT:
# - NEVER enable this option in production environment
# - Use only in isolated test environments
# - Even in development mode use with extreme caution
# - Always verify that agent executes only safe commands
AGENT_ENABLE_SHELL: false

# AGENT_ENABLE_FILE_MANAGEMENT: Allow agent to perform file operations
#
# - true: Agent can create, edit, delete, and move files
#         Useful for organizing knowledge base and managing content
#         Operations include:
#         * Creating new files
#         * Editing existing files
#         * Deleting files
#         * Moving/renaming files
#         * Working with multiple files at once
#
# - false: File operations disabled
#
# IMPORTANT:
# - File operations are restricted to knowledge base directory by default
# - Agent can modify content, so use with caution
# - Recommended: true for knowledge base management
AGENT_ENABLE_FILE_MANAGEMENT: true

# AGENT_ENABLE_FOLDER_MANAGEMENT: Allow agent to perform folder operations
#
# - true: Agent can create, delete, and move folders
#         Useful for organizing knowledge base structure
#         Operations include:
#         * Creating new folders/directories
#         * Deleting folders (with contents)
#         * Moving/renaming folders
#         * Organizing hierarchical structure
#
# - false: Folder operations disabled
#
# IMPORTANT:
# - Folder operations are restricted to knowledge base directory by default
# - Deleting folders will remove all contents
# - Recommended: true for knowledge base organization
AGENT_ENABLE_FOLDER_MANAGEMENT: true


# ───────────────────────────────────────────────────────────────────────────────
# MCP (Model Context Protocol) Settings
# ───────────────────────────────────────────────────────────────────────────────
# MCP enables integration with external tools and services via the Model Context
# Protocol. This allows the agent to access additional capabilities beyond built-in tools.
#
# Documentation: docs_site/agents/mcp-tools.md

# AGENT_ENABLE_MCP: Enable MCP (Model Context Protocol) support
#
# - false: MCP tools disabled (default)
#          Agent uses only built-in tools
#
# - true: Enable MCP tools
#         Allows connecting external tools via MCP protocol
#         MCP server configs are stored in data/mcp_servers/*.json
#
# NOTE: Individual MCP tools must be enabled separately
AGENT_ENABLE_MCP: false

# AGENT_ENABLE_MCP_MEMORY: Enable memory agent tool
#
# - false: Memory agent disabled (default)
#
# - true: Enable local memory management with per-user storage
#         Provides note-taking and search capabilities for the agent
#         Requires:
#         * AGENT_ENABLE_MCP=true
#         * Running scripts/install_mem_agent.py first
#         * Storage type configured via MEM_AGENT_STORAGE_TYPE
#
# Installation:
#   python scripts/install_mem_agent.py
#
# Memory storage:
#   Each user's memory is stored at: data/memory/user_{user_id}/
#   This ensures strict per-user isolation and privacy
#
# MCP servers:
#   Shared servers: data/mcp_servers/ (available to all users)
#   User-specific: data/mcp_servers/user_{user_id}/ (override shared)
AGENT_ENABLE_MCP_MEMORY: false

# MCP_TIMEOUT: Timeout for MCP requests in seconds
#
# - Controls how long to wait for MCP server responses
# - Prevents hanging during long-running MCP operations
# - Recommended values:
#   * Simple requests: 60-120 seconds
#   * Complex operations: 300-600 seconds (5-10 minutes)
#   * Very long operations: 900-1800 seconds (15-30 minutes)
#
# IMPORTANT: Too short timeout may interrupt useful operations
# Default: 600 seconds (10 minutes)
MCP_TIMEOUT: 600


# ───────────────────────────────────────────────────────────────────────────────
# Memory Agent Settings
# ───────────────────────────────────────────────────────────────────────────────
# Configuration for the memory storage system that provides note-taking and search
# capabilities for autonomous agents. Supports multiple storage backends.

# MEM_AGENT_STORAGE_TYPE: Type of memory storage to use
#
# Available options:
# - "json": Simple JSON file storage with substring search (RECOMMENDED for most users)
#           * Fast and lightweight
#           * No ML dependencies required
#           * Works immediately without model download
#           * Best for: keyword-based search, simple use cases
#
# - "vector": AI-powered semantic search using embeddings
#           * Semantic similarity search (understands meaning, not just keywords)
#           * Better understanding of complex queries
#           * Requires model download and ML dependencies (transformers, sentence-transformers)
#           * Best for: large memory collections, semantic understanding needed
#
# - "mem-agent": LLM-based intelligent memory with Obsidian-style markdown (ADVANCED)
#           * Uses LLM to reason about memory operations
#           * Maintains Obsidian-style markdown files with wiki-links
#           * Structured memory (user.md, entities/*.md)
#           * Requires LLM model (driaforall/mem-agent or compatible)
#           * Best for: complex scenarios requiring intelligent organization
#
# Default: "json"
MEM_AGENT_STORAGE_TYPE: json

# MEM_AGENT_MODEL: HuggingFace model ID for memory storage
#
# For "vector" storage type:
#   - BAAI/bge-m3 (default): High-quality multilingual embedding model
#   - sentence-transformers/all-MiniLM-L6-v2: Smaller, faster alternative
#
# For "mem-agent" storage type:
#   - driaforall/mem-agent (default): LLM for intelligent memory management
#
# Default: BAAI/bge-m3
MEM_AGENT_MODEL: BAAI/bge-m3

# MEM_AGENT_MODEL_PRECISION: Model quantization precision (for "vector" and "mem-agent" types)
#
# Available options:
# - "4bit": Smallest, fastest, good quality (RECOMMENDED for most users)
# - "8bit": Balanced size and quality
# - "fp16": Full precision, highest quality, requires more resources
#
# Default: 4bit
MEM_AGENT_MODEL_PRECISION: 4bit

# MEM_AGENT_BACKEND: Backend for running the model (for "vector" and "mem-agent" types)
#
# Available options:
# - "auto": Automatically detect best backend (RECOMMENDED)
#           * macOS: Uses MLX if available, otherwise transformers
#           * Linux/Windows: Uses vLLM if available, otherwise transformers
# - "vllm": Use vLLM backend (fast, for Linux/Windows with GPU)
# - "mlx": Use MLX backend (optimized for Apple Silicon)
# - "transformers": Use HuggingFace transformers (fallback, CPU-friendly)
#
# Default: auto
MEM_AGENT_BACKEND: auto

# MEM_AGENT_BASE_URL: OpenAI-compatible endpoint URL for mem-agent
#
# Examples:
# - http://localhost:8001/v1 (local vLLM/SGLang server)
# - http://host.docker.internal:8001/v1 (for Docker)
# - https://api.openai.com/v1 (OpenAI)
#
# Default: null (will use auto-detection for local servers)
# MEM_AGENT_BASE_URL: http://localhost:8001/v1

# MEM_AGENT_OPENAI_API_KEY: API key for the mem-agent endpoint
#
# Use any value (e.g., "lm-studio") for local servers that don't require authentication
# For OpenAI/OpenRouter, use your actual API key
#
# Default: null
# MEM_AGENT_OPENAI_API_KEY: lm-studio

# MEM_AGENT_MAX_TOOL_TURNS: Maximum number of tool execution turns (for "mem-agent" type)
#
# Prevents infinite loops in LLM memory operations
# Only applicable when MEM_AGENT_STORAGE_TYPE is "mem-agent"
#
# Default: 20
MEM_AGENT_MAX_TOOL_TURNS: 20

# MEM_AGENT_TIMEOUT: Timeout for sandboxed code execution in seconds (for "mem-agent" type)
#
# Safety limit for memory operations
# Only applicable when MEM_AGENT_STORAGE_TYPE is "mem-agent"
#
# Default: 20
MEM_AGENT_TIMEOUT: 20

# MEM_AGENT_FILE_SIZE_LIMIT: Maximum file size in bytes (for "mem-agent" type)
#
# Default: 1048576 (1MB) - prevents memory files from growing too large
# Only applicable when MEM_AGENT_STORAGE_TYPE is "mem-agent"
MEM_AGENT_FILE_SIZE_LIMIT: 1048576

# MEM_AGENT_DIR_SIZE_LIMIT: Maximum directory size in bytes (for "mem-agent" type)
#
# Default: 10485760 (10MB) - limits total size of entities directory
# Only applicable when MEM_AGENT_STORAGE_TYPE is "mem-agent"
MEM_AGENT_DIR_SIZE_LIMIT: 10485760

# MEM_AGENT_MEMORY_SIZE_LIMIT: Maximum total memory size in bytes (for "mem-agent" type)
#
# Default: 104857600 (100MB) - limits total size of all memory data
# Only applicable when MEM_AGENT_STORAGE_TYPE is "mem-agent"
MEM_AGENT_MEMORY_SIZE_LIMIT: 104857600


# ───────────────────────────────────────────────────────────────────────────────
# Vector Search Settings (Advanced Feature)
# ───────────────────────────────────────────────────────────────────────────────
# Vector search enables semantic search capabilities in the knowledge base.
# It finds documents by meaning, not just keyword matching.
#
# ⚠️ REQUIREMENTS:
# Install vector search dependencies first:
#   pip install sentence-transformers faiss-cpu qdrant-client
# Or install all optional dependencies:
#   pip install -e '.[vector-search]'

# VECTOR_SEARCH_ENABLED: Enable vector search functionality
#
# - false: Vector search disabled (default)
#          Knowledge base works with regular text search only
#
# - true: Enable semantic vector search
#         Requires additional dependencies and configuration
#         Allows finding documents by semantic similarity
#
# NOTE: When enabled, knowledge base will be indexed on first use
# Indexing may take time depending on KB size
VECTOR_SEARCH_ENABLED: false

# ─── Embedding Model Settings ──────────────────────────────────────────────────
# Embedding models convert text into numerical vectors for semantic comparison

# VECTOR_EMBEDDING_PROVIDER: Choose embedding model provider
#
# Available options:
#
# - "sentence_transformers": Local embeddings (RECOMMENDED)
#   * Runs on your hardware (CPU or GPU)
#   * No API calls, completely private
#   * Free, no cost per request
#   * Requires: pip install sentence-transformers
#   * Best for: Most use cases, privacy-focused setups
#
# - "openai": OpenAI API embeddings
#   * Requires OPENAI_API_KEY in .env
#   * Paid service (cost per 1000 tokens)
#   * High quality embeddings
#   * Best for: When already using OpenAI services
#
# - "infinity": Infinity API for embeddings
#   * Self-hosted or remote Infinity server
#   * Open-source alternative to OpenAI
#   * Requires running Infinity server
#   * Best for: Custom deployments, API-based workflows
VECTOR_EMBEDDING_PROVIDER: sentence_transformers

# VECTOR_EMBEDDING_MODEL: Specific model name for embeddings
#
# For sentence_transformers:
# - "all-MiniLM-L6-v2": Fast, lightweight (384 dim) - RECOMMENDED
# - "all-mpnet-base-v2": Better quality (768 dim), slower
# - "paraphrase-multilingual-MiniLM-L12-v2": Multilingual support
#
# For OpenAI:
# - "text-embedding-ada-002": Standard model (1536 dim)
# - "text-embedding-3-small": Newer, efficient (1536 dim)
# - "text-embedding-3-large": Best quality (3072 dim)
#
# For Infinity:
# - Model configured in Infinity server
VECTOR_EMBEDDING_MODEL: all-MiniLM-L6-v2

# VECTOR_INFINITY_API_URL: URL for Infinity API server
# Only used when VECTOR_EMBEDDING_PROVIDER="infinity"
# Default: http://localhost:7997
VECTOR_INFINITY_API_URL: http://localhost:7997

# ─── Vector Store Settings ─────────────────────────────────────────────────────
# Vector stores manage and search through embedding vectors

# VECTOR_STORE_PROVIDER: Choose vector database
#
# Available options:
#
# - "faiss": Facebook AI Similarity Search (RECOMMENDED)
#   * Local vector database, runs on your hardware
#   * Very fast search, optimized for similarity queries
#   * No external services needed
#   * Requires: pip install faiss-cpu (or faiss-gpu for GPU)
#   * Best for: Most use cases, local deployments
#
# - "qdrant": Qdrant vector database
#   * Can be local or remote
#   * Production-ready, scalable
#   * Requires running Qdrant server
#   * Best for: Production deployments, distributed systems
VECTOR_STORE_PROVIDER: faiss

# VECTOR_QDRANT_URL: Qdrant server URL
# Only used when VECTOR_STORE_PROVIDER="qdrant"
# Default: http://localhost:6333
VECTOR_QDRANT_URL: http://localhost:6333

# VECTOR_QDRANT_COLLECTION: Qdrant collection name
# Only used when VECTOR_STORE_PROVIDER="qdrant"
# Each knowledge base can have separate collection
VECTOR_QDRANT_COLLECTION: knowledge_base

# ─── Document Chunking Settings ────────────────────────────────────────────────
# Documents are split into chunks before indexing for better search results

# VECTOR_CHUNKING_STRATEGY: How to split documents into chunks
#
# Available strategies:
#
# - "fixed_size": Split into fixed-size pieces
#   * Simple, predictable chunk sizes
#   * May break sentences or paragraphs
#   * Best for: Uniform documents
#
# - "fixed_size_overlap": Fixed size with overlap between chunks
#   * Provides context continuity
#   * Better search accuracy at chunk boundaries
#   * Best for: Most use cases (RECOMMENDED)
#
# - "semantic": Split by document structure (headers, paragraphs)
#   * Preserves semantic meaning
#   * Respects markdown headers and sections
#   * Variable chunk sizes
#   * Best for: Well-structured markdown documents
VECTOR_CHUNKING_STRATEGY: fixed_size_overlap

# VECTOR_CHUNK_SIZE: Size of each chunk in characters
#
# Recommended values:
# - 256-512: Good for short documents, precise matching
# - 512-1024: Balanced, works for most cases (RECOMMENDED: 512)
# - 1024-2048: Better for long-form content, more context
#
# NOTE: Larger chunks = more context but less precise matching
VECTOR_CHUNK_SIZE: 512

# VECTOR_CHUNK_OVERLAP: Overlap between chunks in characters
# Only used with "fixed_size_overlap" strategy
#
# Recommended values:
# - 0: No overlap, more chunks
# - 50-100: Good balance (RECOMMENDED: 50)
# - 100-200: More overlap, better context continuity
#
# NOTE: Must be smaller than VECTOR_CHUNK_SIZE
VECTOR_CHUNK_OVERLAP: 50

# VECTOR_RESPECT_HEADERS: Respect markdown headers when chunking
# Only used with "semantic" strategy
#
# - true: Split by headers, preserve document structure (RECOMMENDED)
# - false: Ignore headers, split by paragraphs only
VECTOR_RESPECT_HEADERS: true

# ─── Search Settings ───────────────────────────────────────────────────────────

# VECTOR_SEARCH_TOP_K: Number of results to return in vector search
#
# Recommended values:
# - 3-5: Quick answers, focused results (RECOMMENDED: 5)
# - 5-10: More comprehensive search
# - 10-20: Exhaustive search, may include less relevant results
VECTOR_SEARCH_TOP_K: 5
