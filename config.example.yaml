# ═══════════════════════════════════════════════════════════════════════════════
# Example Application Configuration
# ═══════════════════════════════════════════════════════════════════════════════
# IMPORTANT: Copy this file to config.yaml and adjust according to your needs
#
# This file contains main application settings that do not include sensitive data.
#
# All sensitive data (tokens, API keys, passwords) should be stored in .env file
# or passed through environment variables to ensure security.
# ═══════════════════════════════════════════════════════════════════════════════


# ───────────────────────────────────────────────────────────────────────────────
# Knowledge Base Settings
# ───────────────────────────────────────────────────────────────────────────────
# The knowledge base is used to store and manage documents processed by the bot.
# Git integration is supported for versioning.

# KB_PATH: Root directory for all knowledge bases
# - This is the base directory where all user knowledge bases are stored
# - Each user's KB is created as a subdirectory: KB_PATH/my-notes, KB_PATH/work-kb, etc.
# - Can be relative (./knowledge_base) or absolute (/path/to/kbs)
# - Directory will be created automatically if it doesn't exist
KB_PATH: ./knowledge_base

# KB_TOPICS_ONLY: Restrict agents to work only in topics/ folder
# - true: Agents can only create/edit/read files in {kb_name}/topics/ directory
#         This prevents agents from modifying index.md, README.md, and other meta files
#         in the knowledge base root. Recommended for production use.
# - false: Agents have full access to entire knowledge base directory
#          Useful for testing or if you need agents to manage meta files
KB_TOPICS_ONLY: true

# KB_GIT_ENABLED: Enable Git integration
# - true: All changes in the knowledge base will be committed to Git repository
# - false: Knowledge base works as regular file system without versioning
KB_GIT_ENABLED: true

# KB_GIT_AUTO_PUSH: Automatically push changes to remote repository
# - true: After each commit, changes are automatically pushed to remote server
# - false: Commits remain local, manual synchronization required
# - IMPORTANT: Only works if KB_GIT_ENABLED=true
KB_GIT_AUTO_PUSH: true

# KB_GIT_REMOTE: Name of the remote Git repository
# - Default "origin" - standard name for the main remote repository
# - Can be changed to another name if using non-standard setup
KB_GIT_REMOTE: origin

# KB_GIT_BRANCH: Git branch for working with knowledge base
# - All commits will be created and pushed to this branch
# - Make sure the branch exists or will be created
KB_GIT_BRANCH: main


# ───────────────────────────────────────────────────────────────────────────────
# Processing Settings
# ───────────────────────────────────────────────────────────────────────────────
# Parameters controlling the logic of processing incoming messages and their grouping.

# MESSAGE_GROUP_TIMEOUT: Message grouping timeout (in seconds)
# - Wait time before processing a group of messages
# - If user sends multiple messages in a row, bot will wait specified number of
#   seconds to group them and process together
# - Recommended value: 30-60 seconds
# - Minimum: 10 seconds, maximum: 300 seconds (5 minutes)
MESSAGE_GROUP_TIMEOUT: 30  # seconds

# PROCESSED_LOG_PATH: Path to the processed messages log file
# - JSON file for tracking history of processed messages
# - Used to prevent duplicate processing and recovery after failures
# - File is created automatically, directory must exist
PROCESSED_LOG_PATH: ./data/processed.json


# ───────────────────────────────────────────────────────────────────────────────
# Logging Settings
# ───────────────────────────────────────────────────────────────────────────────
# Logging system configuration for debugging and monitoring bot operations.

# LOG_LEVEL: Log detail level
# Available values (from least detailed to most detailed):
#
# - CRITICAL: Only critical errors that lead to application shutdown
#
# - ERROR:   Errors that break function operations but don't stop the application
#
# - WARNING: Warnings about potential issues (recommended for production)
#
# - INFO:    General information about application flow (default)
#
# - DEBUG:   Detailed information for debugging, including variables and traces
#            WARNING: May contain sensitive data, do not use in production
#
# Recommendations:
# - Development/debugging: DEBUG or INFO
# - Production: WARNING or ERROR
LOG_LEVEL: INFO

# LOG_FILE: Path to log file
# - All logs are written to this file
# - Automatic file rotation is supported when reaching certain size
# - Make sure directory exists and application has write permissions
# - Log format: timestamp, level, module, message
LOG_FILE: ./logs/bot.log


# ───────────────────────────────────────────────────────────────────────────────
# User Access Control
# ───────────────────────────────────────────────────────────────────────────────
# Security settings to restrict access to the bot.

# ALLOWED_USER_IDS: List of allowed Telegram user IDs
#
# Format:
# - Empty string ("") - access allowed for all users (unsafe for production!)
#
# - Single ID: "123456789"
#
# - Multiple IDs (comma-separated): "123456789,987654321,555555555"
#
# How to find your Telegram ID:
# 1. Message @userinfobot bot in Telegram
# 2. Or use @getmyid_bot
# 3. ID will be shown in response message
#
# IMPORTANT: In production environment ALWAYS specify specific user IDs!
#
# Example for multiple users:
# ALLOWED_USER_IDS: "123456789,987654321"
ALLOWED_USER_IDS: ""


# ───────────────────────────────────────────────────────────────────────────────
# Agent Configuration
# ───────────────────────────────────────────────────────────────────────────────
# AI agent settings that process user requests and perform tasks.

# AGENT_TYPE: Type of agent to use
#
# Available options:
#
# - "stub": Stub for testing without real AI
#          Returns fixed responses, doesn't require API keys
#          Use only for development and testing
#
# - "qwen_code": Agent based on Qwen AI via API
#               Requires QWEN_API_KEY in .env file
#               Less stable, may have issues with tools
#
# - "qwen_code_cli": Agent based on Qwen AI via command line (RECOMMENDED)
#                   Requires installed qwen CLI tool
#                   Most stable option with full tool support
#                   Supports code, files, web search operations
AGENT_TYPE: "qwen_code_cli"

# AGENT_MODEL: AI model to use (for qwen_code type)
#
# Available Qwen models:
#
# - "qwen-max": Most advanced model, best response quality
#              Recommended for complex tasks
#
# - "qwen-plus": Balanced model, good quality/speed ratio
#
# - "qwen-turbo": Fast model for simple tasks
#
# NOTE: This setting is only used if AGENT_TYPE="qwen_code"
# For qwen_code_cli the model is set in CLI tool configuration
AGENT_MODEL: "qwen-max"

# AGENT_QWEN_CLI_PATH: Path to qwen CLI executable
#
# - "qwen": CLI available through PATH (recommended)
#
# - "/usr/local/bin/qwen": Absolute path to executable
#
# - "./bin/qwen": Relative path from project root
#
# Installing qwen CLI:
# 1. Follow instructions at https://github.com/QwenLM/Qwen-Agent
# 2. After installation check: qwen --version
#
# NOTE: Only used if AGENT_TYPE="qwen_code_cli"
AGENT_QWEN_CLI_PATH: "qwen"

# AGENT_TIMEOUT: Maximum time for agent operation execution (in seconds)
#
# - Prevents hanging during long operations
#
# - If operation exceeds this time, it will be interrupted
#
# - Recommended values:
#   * Simple requests: 60-120 seconds
#   * Complex tasks with code: 300-600 seconds (5-10 minutes)
#   * Very complex processing: 900-1800 seconds (15-30 minutes)
#
# IMPORTANT: Too short timeout may interrupt useful operations
AGENT_TIMEOUT: 300

# AGENT_ENABLE_WEB_SEARCH: Allow agent to use web search
#
# - true: Agent can search information online to answer questions
#         Useful for getting up-to-date information and reference data
#
# - false: Agent works only with local data
#          More secure, but limits capabilities
#
# NOTE: Requires internet connection and may increase response time
AGENT_ENABLE_WEB_SEARCH: true

# AGENT_ENABLE_GIT: Allow agent to work with Git repositories
#
# - true: Agent can perform Git operations (clone, pull, commit, push, etc.)
#         Required for working with versioning and repositories
#
# - false: Git functions disabled
#
# IMPORTANT: Make sure Git is installed in the system and configured
# (git config user.name and git config user.email must be set)
AGENT_ENABLE_GIT: true

# AGENT_ENABLE_GITHUB: Allow agent to interact with GitHub API
#
# - true: Agent can work with GitHub (create issues, PRs, read code, etc.)
#         Requires GITHUB_TOKEN in .env file for authorization
#
# - false: GitHub integration disabled
#
# Usage:
# - Creating and managing issues
# - Working with pull requests
# - Reading code from public and private repositories
# - Analyzing commit history
AGENT_ENABLE_GITHUB: true

# AGENT_ENABLE_SHELL: Allow agent to execute shell commands
#
# ⚠️ CRITICAL SECURITY WARNING ⚠️
#
# - true: Agent can execute ANY commands in the operating system
#         EXTREMELY DANGEROUS! Can lead to:
#         * File and data deletion
#         * System compromise
#         * Unauthorized access
#         * Malicious code execution
#
# - false: Shell commands forbidden (RECOMMENDED)
#
# IMPORTANT:
# - NEVER enable this option in production environment
# - Use only in isolated test environments
# - Even in development mode use with extreme caution
# - Always verify that agent executes only safe commands
AGENT_ENABLE_SHELL: false

# AGENT_ENABLE_FILE_MANAGEMENT: Allow agent to perform file operations
#
# - true: Agent can create, edit, delete, and move files
#         Useful for organizing knowledge base and managing content
#         Operations include:
#         * Creating new files
#         * Editing existing files
#         * Deleting files
#         * Moving/renaming files
#         * Working with multiple files at once
#
# - false: File operations disabled
#
# IMPORTANT:
# - File operations are restricted to knowledge base directory by default
# - Agent can modify content, so use with caution
# - Recommended: true for knowledge base management
AGENT_ENABLE_FILE_MANAGEMENT: true

# AGENT_ENABLE_FOLDER_MANAGEMENT: Allow agent to perform folder operations
#
# - true: Agent can create, delete, and move folders
#         Useful for organizing knowledge base structure
#         Operations include:
#         * Creating new folders/directories
#         * Deleting folders (with contents)
#         * Moving/renaming folders
#         * Organizing hierarchical structure
#
# - false: Folder operations disabled
#
# IMPORTANT:
# - Folder operations are restricted to knowledge base directory by default
# - Deleting folders will remove all contents
# - Recommended: true for knowledge base organization
AGENT_ENABLE_FOLDER_MANAGEMENT: true


# ───────────────────────────────────────────────────────────────────────────────
# MCP (Model Context Protocol) Settings
# ───────────────────────────────────────────────────────────────────────────────
# MCP enables integration with external tools and services via the Model Context
# Protocol. This allows the agent to access additional capabilities beyond built-in tools.
#
# Documentation: docs_site/agents/mcp-tools.md

# AGENT_ENABLE_MCP: Enable MCP (Model Context Protocol) support
#
# - false: MCP tools disabled (default)
#          Agent uses only built-in tools
#
# - true: Enable MCP tools
#         Allows connecting external tools via MCP protocol
#         MCP server configs are stored in data/mcp_servers/*.json
#
# NOTE: Individual MCP tools must be enabled separately
AGENT_ENABLE_MCP: false

# AGENT_ENABLE_MCP_MEMORY: Enable memory agent tool
#
# - false: Memory agent disabled (default)
#
# - true: Enable local memory management with mem-agent
#         Uses HuggingFace model for intelligent memory storage
#         Requires:
#         * AGENT_ENABLE_MCP=true
#         * Running scripts/install_mem_agent.py first
#         * Model: BAAI/bge-m3 from HuggingFace
#
# Installation:
#   python scripts/install_mem_agent.py
#
# Memory storage:
#   Each user's memory is stored in their KB at: {kb_path}/memory/
#   This ensures per-user isolation and privacy
AGENT_ENABLE_MCP_MEMORY: false


# ───────────────────────────────────────────────────────────────────────────────
# Memory Agent Settings (Local LLM)
# ───────────────────────────────────────────────────────────────────────────────
# Configuration for the local mem-agent that provides intelligent memory management
# using a locally-run LLM model. No API keys required.

# MEM_AGENT_STORAGE_TYPE: Type of memory storage to use
#
# Available options:
# - "json": Simple JSON file storage with substring search (RECOMMENDED for most users)
#           * Fast and lightweight
#           * No ML dependencies required
#           * Works immediately without model download
# - "model": AI-powered semantic search using embeddings
#           * Semantic similarity search
#           * Better understanding of queries
#           * Requires model download and ML dependencies (transformers, sentence-transformers)
#
# Default: "json"
MEM_AGENT_STORAGE_TYPE: "json"

# MEM_AGENT_MODEL: HuggingFace model ID for memory agent
#
# Default: BAAI/bge-m3
# This is an embedding model for vector-based semantic search
# Only used when MEM_AGENT_STORAGE_TYPE is set to "model"
MEM_AGENT_MODEL: "BAAI/bge-m3"

# MEM_AGENT_MODEL_PRECISION: Model quantization precision
#
# Available options:
# - "4bit": Smallest, fastest, lowest quality (RECOMMENDED for most users)
# - "8bit": Balanced size and quality
# - "fp16": Full precision, highest quality, requires more resources
MEM_AGENT_MODEL_PRECISION: "4bit"

# MEM_AGENT_BACKEND: Backend for running the model
#
# Available options:
# - "auto": Automatically detect best backend (RECOMMENDED)
#           * macOS: Uses MLX if available, otherwise transformers
#           * Linux/Windows: Uses vLLM if available, otherwise transformers
# - "vllm": Use vLLM backend (fast, for Linux/Windows)
# - "mlx": Use MLX backend (optimized for Apple Silicon)
# - "transformers": Use HuggingFace transformers (fallback, slower)
MEM_AGENT_BACKEND: "auto"

# MEM_AGENT_VLLM_HOST: vLLM server host (only used if backend is vllm)
MEM_AGENT_VLLM_HOST: "127.0.0.1"

# MEM_AGENT_VLLM_PORT: vLLM server port (only used if backend is vllm)
MEM_AGENT_VLLM_PORT: 8001

# MEM_AGENT_MEMORY_POSTFIX: Directory name for memory storage within each user's KB
#
# Default: "memory"
# Full path will be: {kb_path}/memory/
# Each user gets their own isolated memory directory
MEM_AGENT_MEMORY_POSTFIX: "memory"

# MEM_AGENT_MAX_TOOL_TURNS: Maximum number of tool execution turns
#
# Prevents infinite loops in memory operations
MEM_AGENT_MAX_TOOL_TURNS: 20

# MEM_AGENT_TIMEOUT: Timeout for sandboxed code execution (seconds)
#
# Safety limit for memory operations
MEM_AGENT_TIMEOUT: 20

# MEM_AGENT_FILE_SIZE_LIMIT: Maximum file size in bytes
#
# Default: 1MB - prevents memory files from growing too large
MEM_AGENT_FILE_SIZE_LIMIT: 1048576

# MEM_AGENT_DIR_SIZE_LIMIT: Maximum directory size in bytes
#
# Default: 10MB - limits total size of entities directory
MEM_AGENT_DIR_SIZE_LIMIT: 10485760

# MEM_AGENT_MEMORY_SIZE_LIMIT: Maximum total memory size in bytes
#
# Default: 100MB - limits total size of all memory data
MEM_AGENT_MEMORY_SIZE_LIMIT: 104857600


# ───────────────────────────────────────────────────────────────────────────────
# Vector Search Settings (Advanced Feature)
# ───────────────────────────────────────────────────────────────────────────────
# Vector search enables semantic search capabilities in the knowledge base.
# It finds documents by meaning, not just keyword matching.
#
# ⚠️ REQUIREMENTS:
# Install vector search dependencies first:
#   pip install sentence-transformers faiss-cpu qdrant-client
# Or install all optional dependencies:
#   pip install -e '.[vector-search]'

# VECTOR_SEARCH_ENABLED: Enable vector search functionality
#
# - false: Vector search disabled (default)
#          Knowledge base works with regular text search only
#
# - true: Enable semantic vector search
#         Requires additional dependencies and configuration
#         Allows finding documents by semantic similarity
#
# NOTE: When enabled, knowledge base will be indexed on first use
# Indexing may take time depending on KB size
VECTOR_SEARCH_ENABLED: false

# ─── Embedding Model Settings ──────────────────────────────────────────────────
# Embedding models convert text into numerical vectors for semantic comparison

# VECTOR_EMBEDDING_PROVIDER: Choose embedding model provider
#
# Available options:
#
# - "sentence_transformers": Local embeddings (RECOMMENDED)
#   * Runs on your hardware (CPU or GPU)
#   * No API calls, completely private
#   * Free, no cost per request
#   * Requires: pip install sentence-transformers
#   * Best for: Most use cases, privacy-focused setups
#
# - "openai": OpenAI API embeddings
#   * Requires OPENAI_API_KEY in .env
#   * Paid service (cost per 1000 tokens)
#   * High quality embeddings
#   * Best for: When already using OpenAI services
#
# - "infinity": Infinity API for embeddings
#   * Self-hosted or remote Infinity server
#   * Open-source alternative to OpenAI
#   * Requires running Infinity server
#   * Best for: Custom deployments, API-based workflows
VECTOR_EMBEDDING_PROVIDER: "sentence_transformers"

# VECTOR_EMBEDDING_MODEL: Specific model name for embeddings
#
# For sentence_transformers:
# - "all-MiniLM-L6-v2": Fast, lightweight (384 dim) - RECOMMENDED
# - "all-mpnet-base-v2": Better quality (768 dim), slower
# - "paraphrase-multilingual-MiniLM-L12-v2": Multilingual support
#
# For OpenAI:
# - "text-embedding-ada-002": Standard model (1536 dim)
# - "text-embedding-3-small": Newer, efficient (1536 dim)
# - "text-embedding-3-large": Best quality (3072 dim)
#
# For Infinity:
# - Model configured in Infinity server
VECTOR_EMBEDDING_MODEL: "all-MiniLM-L6-v2"

# VECTOR_INFINITY_API_URL: URL for Infinity API server
# Only used when VECTOR_EMBEDDING_PROVIDER="infinity"
# Default: http://localhost:7997
VECTOR_INFINITY_API_URL: "http://localhost:7997"

# ─── Vector Store Settings ─────────────────────────────────────────────────────
# Vector stores manage and search through embedding vectors

# VECTOR_STORE_PROVIDER: Choose vector database
#
# Available options:
#
# - "faiss": Facebook AI Similarity Search (RECOMMENDED)
#   * Local vector database, runs on your hardware
#   * Very fast search, optimized for similarity queries
#   * No external services needed
#   * Requires: pip install faiss-cpu (or faiss-gpu for GPU)
#   * Best for: Most use cases, local deployments
#
# - "qdrant": Qdrant vector database
#   * Can be local or remote
#   * Production-ready, scalable
#   * Requires running Qdrant server
#   * Best for: Production deployments, distributed systems
VECTOR_STORE_PROVIDER: "faiss"

# VECTOR_QDRANT_URL: Qdrant server URL
# Only used when VECTOR_STORE_PROVIDER="qdrant"
# Default: http://localhost:6333
VECTOR_QDRANT_URL: "http://localhost:6333"

# VECTOR_QDRANT_COLLECTION: Qdrant collection name
# Only used when VECTOR_STORE_PROVIDER="qdrant"
# Each knowledge base can have separate collection
VECTOR_QDRANT_COLLECTION: "knowledge_base"

# ─── Document Chunking Settings ────────────────────────────────────────────────
# Documents are split into chunks before indexing for better search results

# VECTOR_CHUNKING_STRATEGY: How to split documents into chunks
#
# Available strategies:
#
# - "fixed_size": Split into fixed-size pieces
#   * Simple, predictable chunk sizes
#   * May break sentences or paragraphs
#   * Best for: Uniform documents
#
# - "fixed_size_overlap": Fixed size with overlap between chunks
#   * Provides context continuity
#   * Better search accuracy at chunk boundaries
#   * Best for: Most use cases (RECOMMENDED)
#
# - "semantic": Split by document structure (headers, paragraphs)
#   * Preserves semantic meaning
#   * Respects markdown headers and sections
#   * Variable chunk sizes
#   * Best for: Well-structured markdown documents
VECTOR_CHUNKING_STRATEGY: "fixed_size_overlap"

# VECTOR_CHUNK_SIZE: Size of each chunk in characters
#
# Recommended values:
# - 256-512: Good for short documents, precise matching
# - 512-1024: Balanced, works for most cases (RECOMMENDED: 512)
# - 1024-2048: Better for long-form content, more context
#
# NOTE: Larger chunks = more context but less precise matching
VECTOR_CHUNK_SIZE: 512

# VECTOR_CHUNK_OVERLAP: Overlap between chunks in characters
# Only used with "fixed_size_overlap" strategy
#
# Recommended values:
# - 0: No overlap, more chunks
# - 50-100: Good balance (RECOMMENDED: 50)
# - 100-200: More overlap, better context continuity
#
# NOTE: Must be smaller than VECTOR_CHUNK_SIZE
VECTOR_CHUNK_OVERLAP: 50

# VECTOR_RESPECT_HEADERS: Respect markdown headers when chunking
# Only used with "semantic" strategy
#
# - true: Split by headers, preserve document structure (RECOMMENDED)
# - false: Ignore headers, split by paragraphs only
VECTOR_RESPECT_HEADERS: true

# ─── Search Settings ───────────────────────────────────────────────────────────

# VECTOR_SEARCH_TOP_K: Number of results to return in vector search
#
# Recommended values:
# - 3-5: Quick answers, focused results (RECOMMENDED: 5)
# - 5-10: More comprehensive search
# - 10-20: Exhaustive search, may include less relevant results
VECTOR_SEARCH_TOP_K: 5
