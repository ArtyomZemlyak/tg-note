# Dockerfile for vLLM Server
# Provides OpenAI-compatible API for LLM inference

FROM vllm/vllm-openai:latest

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    HF_HOME=/root/.cache/huggingface

# Install additional dependencies if needed
RUN pip install --no-cache-dir \
    huggingface-hub

# Expose vLLM API port
EXPOSE 8001

# Health check endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=5 \
    CMD curl -f http://localhost:8001/health || exit 1

# Start vLLM server with environment-based configuration
# Default model: driaforall/mem-agent
# Override with MODEL_NAME environment variable
CMD ["sh", "-c", "\
    vllm serve ${MODEL_NAME:-driaforall/mem-agent} \
    --host 0.0.0.0 \
    --port 8001 \
    --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.8} \
    --max-model-len ${MAX_MODEL_LEN:-4096} \
    --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}"]
